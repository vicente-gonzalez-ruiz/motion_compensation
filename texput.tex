\title{Video compression fundamentals}
\author{Vicente González Ruiz}
\maketitle
\tableofcontents

\section{\mylink{redundancy}{Sources of redundancy}}

\subsection{Intra (I) video coding}

In the III... (or Intra video) coding, the
\href{https://sistemas-multimedia.github.io/milestones/07-DCT/}{2D
  block-DWT}, the
\href{https://sistemas-multimedia.github.io/milestones/08-DWT/}{2D
  DWT}, or any other spatial transform, is used on sequences of frames
(images) to exploit the spatial correlation. This is achieved by
simply iterating the spatial decorrelation as it is described in the
Algorithm~\ref{alg:III_coding}~\cite{taubman2002jpeg2000}, where $V$
in the input sequence and $S$ controls the number of SRLs (Spatial
Resolution Levels)\footnote{Notice that at least one SRL is always
available for each image or video sequence}. The synthesis transform
is computed using the Algorithm~\ref{alg:III_decoding}. In the
Fig.~\ref{fig:III} there is an example of the decomposition generated
for three frames $V_0$, $V_1$ and $V_2$.

\begin{myalg}{III-coding}{$\mathbf{V}$ /* original video sequence */, $S$ /* Number of extra levels */}{$\mathbf{O}$ /* transformed video sequence */}
  \label{alg:III_coding}
  \begin{enumerate}
  \item ${\mathbf O}=\{\}$ /* empty sequence */.
  \item for ${\mathbf V}_i\in {\mathbf V}$:
    \begin{enumerate}
    \item ${\mathbf O}_i\leftarrow\text{2D-T}^{S}({\mathbf V}_i)$ /* 2D analysis spatial transform */.
    \end{enumerate}
  \end{enumerate}
\end{myalg}

\begin{myalg}{III-decoding}{$\mathbf{O}$ /* transformed video sequence */, $S$ /* Number of extra levels */}{$\mathbf{V}$ /* original video sequence */}
  \label{alg:III_decoding}
  \begin{enumerate}
  \item ${\mathbf V}=\{\}$ /* empty sequence */.
  \item for ${\mathbf O}_i\in {\mathbf O}$:
    \begin{enumerate}
    \item ${\mathbf V}_i\leftarrow\text{2D-T}^{-S}({\mathbf O}_i)$ /* 2D synthesis spatial transform */.
    \end{enumerate}
  \end{enumerate}
\end{myalg}

\begin{figure}
  \centering
  \myfig{graphics/forward_MDWT}{6cm}{600}
  \caption{Decomposition generated by 1-levels ($S=1$) 2D-DWT and the 2x2-DCT (block size is equal to $S+1$).}
  \label{fig:III}
\end{figure}

\section{Hybrid video coding}

\begin{figure}
  \svg{hybrid_coding}{800}
  \caption{Hybrid video coding.}
  \label{fig:hybrid_coding}
\end{figure}

See the Fig.~\ref{fig:hybrid_coding}.

\begin{itemize}
\tightlist
\item
  Used in all the video compression standards.
\item
  Intracoded images are transformed ($T$) and quantized ($Q$).
\item
  Intercoded images are also motion compensated ($P$).
\item
  The encoder incorporates a decoder to avoid the drift error.
\end{itemize}


\subsection{Motion compensation prediction (P) video coding}
III... video coding is fast and good for video edition, but the
compression ratios are poor because the temporal correlation is not
removed. IPP... video coding (also called inter-coding) uses MC
(Motion Compensation) to exploit the temporal correlation in frame
sequences. IPP... encoders split the sequence into
\href{https://en.wikipedia.org/wiki/Group_of_pictures}{GOPs (Groups of
  Frames)}, where the first frame of each GOP is a I-type frame, and
the rest of frames of the GOP are P-type. P-frames are
motion-compensated frames and usually the reference frame is the
previous one, in the temporal order.

\subsection{Types of macro-blocks}
A P-type frame is created in two steps:
\begin{enumerate}
\item The frame is divided into non-overlapping blocks (typically of
  $16\times 16$ pixels), that in this context we will call,
  macro-blocks.
\item A prediction frame is generated using a field of motion vectors
  (one vector per matro-block) and a reference.
\item The prediction frame is substracted to the predicted frame,
  generating a prediction-error frame.
\end{enumerate}
In general, most of the time this procedure success in temporally
decorrelate the frames. However, it can also happen that when the
reference(s) frame(s) is very different from the predicted one, the
prediction-error frame has a higher entropy than the original
(predicted) one.

In order to minimize this inconvenient, most video coding standards
use the following types of macro-blocks:
\begin{enumerate}
\item I (intra): if the macro-block is not compensated.
\item P (predicted): if the macro-blocks is compensated.
\item B (bidirectionally predicted): if the macro-block is compensated
  using an anterior and a posterior frame.
\item S (skipped): if the macro-block is substituted by reference one
  (no substraction is performed).
\end{enumerate}
The decision of the type of each macro-block is performed at the
compressor, and this information is signaled in the code-stream. An
visual example of the decision of the type of the macro-blocks is
shown in the Figure~\ref{fig:macroblocks}.

\begin{figure}
  \centering
  \myfig{graphics/macroblocks}{6cm}{600}
  \caption{Types of macro-blocks.}
  \label{fig:macroblocks}
\end{figure}

\subsection{Macro-block type decision}
Ideally, the type (also called, mode) of the macro-blocks is decided
considering all the posibilities (I, P, B, or S) and selecting the
most beneficial one from a RD perpective, i.e., selecting the
alternative that minimizes the RD cost. This RDO is performed at the
macro-block level.

\subsection{Codec}
\begin{figure}
  \centering
    \svg{graphics/codec}{1100}
  \caption{A inter/intra video codec.}
\label{fig:IPP_codec}
\end{figure}

The Figure~\ref{fig:IPP_codec} describes an inter/intra video codec,
that corresponds to:

\begin{equation}
  {\mathbf W}_k = \text{color-T}({\mathbf V}_k),
  \tag{a}
\end{equation}

\begin{equation}
   {\mathbf W}_{k-1} = Z^{-1}({\mathbf W}, k-1),
  \tag{b}
\end{equation}
and by definition, $Z^{-1}({\mathbf W}, -1) = {\mathbf 0}$,

\begin{equation}
  \overset{(k-1)\rightarrow k}{\mathbf M} = \text{ME}({\mathbf W}_{k-1}, {\mathbf W}_k),
  \tag{c}
\end{equation}
where ME stands for Motion Estimation, and by definition,
$\overset{(-1)\rightarrow 0}{{\mathbf M}} = {\mathbf 0}$,

\begin{equation}
  \overset{(k-1)\rightarrow k}{\mathbf M} = \overset{(k-1)\rightarrow k}{\mathbf M} \text{(lossless~coding)},
  \tag{d}
\end{equation}

\begin{equation}
  \overset{(k-1)\rightarrow k}{\mathbf M} = \overset{(k-1)\rightarrow k}{\mathbf M} \text{(lossless~decoding)},
  \tag{e}
\end{equation}

\begin{equation}
  {\mathbf E}_k = {\mathbf W}_k - \overset{\wedge}{{\mathbf W}}_k,
  \tag{f}
\end{equation}
where the symbol $-$ represents to the pixel-wise substraction,

\begin{equation}
  \overset{\sim}{{\mathbf E}_k} = \text{QE}({\mathbf E}_k),
  \tag{g}
\end{equation}
where QE$(\cdot)$ represents the lossy compression of the
prediction error texture data,

\begin{equation}
  \overset{\sim}{\mathbf E}_k = \text{DQ}^{-1}(\overset{\sim}{\mathbf E}_k),
  \tag{h}
\end{equation}
where DQ$^{-1}(\cdot)$ represents the decompression of the prediction
error texture data,

\begin{equation}
  \overset{\sim}{\mathbf W}_k \leftarrow \overset{\sim}{\mathbf E}_k + \overset{\wedge}{\mathbf W}_k,
  \tag{i}
\end{equation}
and notice that if $\overset{\wedge}{\mathbf W}_k = {\mathbf 0}$, then
$\overset{\sim}{\mathbf E}_k = \overset{\sim}{\mathbf W}_k$,

\begin{equation}
  \overset{\wedge}{\mathbf W}_k = \text{P}(\overset{\sim}{\mathbf W}_{k-1}, \overset{(k-1)\rightarrow k}{\mathbf M}),
  \tag{j}
\end{equation}
where P$(\cdot,\cdot)$ is a motion compensated predictor, and

\begin{equation}
   \overset{\wedge}{\mathbf W}_{k-1} = Z^{-1}(\overset{\wedge}{\mathbf W}, k-1),
  \tag{k}
\end{equation}
where by definition, $Z^{-1}(\overset{\wedge}{\mathbf W}, -1) = 0$, and

\begin{equation}
   {\mathbf V} = \text{color-T}^{-1}({\mathbf W}_k),
  \tag{l}
\end{equation}
is the inverse color transform.

Notice that if $\overset{\wedge}{{\mathbf W}}_k$ is similar to
${\mathbf W}_k$, then ${\mathbf E}_k$ will be approximately zero, and
therefore, easely compressed. Another interesting aspect to highlight
is that the encoder replicates de decoder in order to use the
reconstructed images as reference and avoid the drift error.

\section{Block-based MC (Motion Compensation)~\cite{rao1996techniques}}
\begin{itemize}
\tightlist
\item
  Usually, only performed by the encoder (compress one. decompress
  many).
\item
  MC removes temporal redundancy. A \emph{predicted image} can be
  encoded as the difference between it and another image called
  \emph{prediction image} which is a motion compensated projection of
  one or more images named \emph{reference images}. ME tries to
  generate \emph{residue images} as close as possible to the null
  images.
\item
  For example, in the MPEG-1 standard, the reference image/s is/are
  divided in blocks of $16\times 16$ pixels called \emph{macroblocks}.
\item
  Each reference block is searched in the predicted image and the best
  match is indicated by mean of a \emph{motion vector}.
\item
  Depending on the success of the search and the number of reference
  images, the macroblocks are classified into:

  \begin{itemize}
  \tightlist
  \item
    \textbf{I (intra)}: When the compression of residue block generates
    more bits than the original (predicted) one.
  \item
    \textbf{P (predicted)}: When it is better to compress the residue
    block and there is only one reference macroblock.
  \item
    \textbf{B (bidirectionally predicted)}: The same, but if we have two
    reference macroblocks.
  \item
    \textbf{S (skipped)}: When the energy of the residue block is
    smaller than a given threshold.
  \end{itemize}
\item
  I-pictures are composed of I macroblocks, only.
\item
  P-pictures do not have B macrobocks.
\item
  B-pictures can have any type of macroblocks.
\end{itemize}
See the Fig.~\ref{fig:macroblocks}.

\begin{figure}
  \svg{macroblocks}{600}
  \caption{Types of macroblocks.}
  \label{fig:macroblocks}
\end{figure}

\section{Sub-pixel accuracy}

\begin{itemize}
\tightlist
\item
  The motion estimation can be carried out using integer pixel accuracy
  or a fractional (sub-) pixel accuracy.
\item
  For example, in MPEG-1, the motion estimation can have up to 1/2
  pixel accuracy. A bi-linear interpolator is used (see the
  Fig.~\ref{fig:interpolation}).
\end{itemize}

\begin{figure}
  \svg{interpolation}{200}
  \caption{Pixel interpolation.}
  \label{fig:interpolation}
\end{figure}

\section{Matching criteria (similitude between macroblocks)}
\begin{itemize}
\item
  Let $a$ and $b$ the macroblocks which we want to compare. Two main
  distortion metrics are commonly used:

  \begin{itemize}
  \item
    \textbf{MSE (Mean Square Error)}:

    \begin{equation}
      \frac{1}{16\times 16}\sum_{i=1}^{16}\sum_{j=1}^{16}(a_{ij}-b_{ij})^2
    \end{equation}
  \item
    \textbf{MAE (Mean Absolute Error)}:

    \begin{equation}
      \frac{1}{16\times 16}\sum_{i=1}^{16}\sum_{j=1}^{16}|a_{ij}-b_{ij}|
    \end{equation}
  \end{itemize}
\item
  These similitude measures are used only by MPEG compressors.
  Therefore, any other one with similar effects (such as the error
  variance or the error entropy) could be used also.
\item
  Other less common distortion metrics that can work are:

  \begin{itemize}
  \item
    \textbf{EE (Error
    \href{https://en.wikipedia.org/wiki/Entropy_(information_theory)}{Entropy}}:

    \begin{equation}
      -\frac{1}{16\times 16}\sum_{i=1}^{16}\sum_{j=1}^{16}\log_2(a_{ij}-b_{ij})p(a_{ij}-b_{ij})
    \end{equation}
  \end{itemize}
\end{itemize}

\section{Searching strategies}
\begin{itemize}
\item
  Only performed by the compressor.

  \begin{itemize} \tightlist \item \textbf{Full search}: All the
  possibilities are checked (see the
  Fig.~\ref{fig:full_search}). Advantage: the best
  compression. Disadvantage: CPU killer.  \end{itemize}

  \begin{figure}
    \svg{full_search}{500}
    \caption{The full search scheme.}
    \label{fig:full_search}
  \end{figure}
  
  \begin{itemize}
  \tightlist
  \item
    ** Logaritmic search**: It is a version of the full search algorithm
    where the macro-blocks and the search area are sub-sampled. After
    finding the best coincidence, the resolution is increased in a power
    of 2 and the previous match is refined in a search area of
    $\pm 1$, until the maximal resolution (even using subpixel
    accuracy) is reached.
  \end{itemize}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Telescopic search}: Any of the previously described
    techniques can be speeded up if the searching area is reduced. This
    can be done supposing that the motion vector of the same macro-block
    in two consecutive images is similar.
  \end{itemize}
\end{itemize}

\section{The GOP (Group Of Pictures) concept}
\begin{itemize}
\tightlist
\item
  The temporal redundancy is exploited by blocks of images called GOPs.
  This means that a GOP can be decoded independently of the rest of
  GOPs (see the Fig.~\ref{fig:GOPs}).
\end{itemize}

\begin{figure}
  \svg{GOPs}{500}
  \caption{A GOP.}
  \label{fig:GOPs}
\end{figure}

\section{MCTF (Motion Compensated Temporal Filtering)}
\begin{itemize}
\tightlist
\item
  This is a DWT where the input samples are the original video images
  and the output is a sequence of residue images.
\end{itemize}

\section{$\pm$ 1-spiral-search ME (Motion Estimation)}
\begin{figure}[h]
  \svg{spiral_search}{500}
  \caption{Spiral search.}
  \label{fig:spiral_search}
\end{figure}

\section{Linear frame interpolation using block-based motion compensation}
\label{sec:linear_frame_interpolation}
\begin{figure}[h]
  \svg{frame_interpolation}{900}
  \caption{Frame interpolation.}
  \label{fig:frame_interpolation}
\end{figure}

\subsection*{Input}
\begin{itemize}
\tightlist
\item
  $R$: square search area, in pixels.
\item
  $B$: square block size, in pixels.
\item
  $O$: border size, in pixels.
\item
  $s_i$, $s_j$ and $s_k$ three chronologically ordered,
  equidistant frames, with resolution $X\times Y$.
\item
  $A$: $\frac{1}{2^A}$ subpixel accuracy.
\end{itemize}

\subsection*{Output}
\begin{itemize}
\tightlist
\item
  $\hat{s}_j$: a prediction for frame $s_j$.
\item
  $m$: a matrix with $\lceil X/B\rceil \times \lceil Y/B\rceil$
  bidirectional motion vectors.
\item
  $e$: a matrix with $\lceil X/B\rceil \times \lceil Y/B\rceil$
  bidirectional Root Mean Square matching Wrrors (RMSE).
\end{itemize}

\subsection*{Algorithm}
\begin{enumerate}
\tightlist

\item
  Compute the DWT$^l$, where $l=\lfloor\log_2(R)\rfloor-1$ levels,
  of the predicted frame $s_j$ and the two reference frames $s_i$
  and $s_k$.
  \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_1.svg}{Example}.

\item
  $LL^l(m)\leftarrow 0$, or any other precomputed values (for example,
  from a previous ME in neighbor frames).
  \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_2.svg}{Example}.

\item
  Divide the subband $LL^l(s_j)$ into blocks of size $B\times B$
  pixels, and $\pm 1$-spiral-search them in the subbands $LL^l(s_i)$
  and $LL^l(s_k)$, calculating a low-resolution
  $LL^l(m)=\{LL^l(\overleftarrow{m}), LL^l(\overrightarrow{m})\}$
  bi-directional motion vector field. 
  \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_3A.svg}{Example}.
  \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_3A_bis.svg}{Example}.
\item
  While $l>0$:
  
  \begin{enumerate}
    
  \item
    Synthesize $LL^{l-1}(m)$, $LL^{l-1}(s_j)$, $LL^{l-1}(s_i)$
    and $LL^{l-1}(s_k)$, by computing the 1-level DWT$^{-1}$.
    \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_4A.svg}{Example}.
    \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_4A_bis.svg}{Example}

  \item
    $LL^{l-1}(M)\leftarrow LL^{l-1}(M)\times 2$.
    \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_4B.svg}{Example}.
  
  \item
    Refine $LL^{l-1}(m)$ using $\pm 1$-spiral-search.
    \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_4C.svg}{Example}.
  
  \item
  $l\leftarrow l-1$. (When $l=0$, the motion vectors field $m$
  has the structure:)
  
  \end{enumerate}
  
  \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/motion_vectors.svg}{Example}.
  
\item
  While $l<A$ (in the first iteration, $l=0$, and $LL^0(M):=M$):
  
  \begin{enumerate}
  \item
    $l\leftarrow l+1$.
    
  \item
    Synthesize $LL^{-l}(s_j)$, $LL^{-l}(s_i)$ and $LL^{-l}(s_k)$,
    computing the 1-level DWT$^{-1}$ (high-frequency subbands are
    $0$). This performs a zoom-in in these frames using $1/2$-subpixel
    accuracy. 
    
    \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_5B.svg}{Example}.
    
  \item
    $m\leftarrow m\times 2$.
    
    \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/motion_vectors_by_2.svg}{Example}.
    
  \item
    $B\leftarrow B\times 2$.
    
  \item
    Divide the subband $LL^{-l}(s_j)$ into blocks of $B\times B$
    pixels and $\pm 1$-spiral-search them into the subbands
    $LL^{-l}(s_i)$ and $LL^{-l}(s_k)$, calculating a $1/2^l$
    sub-pixel accuracy $m$ bi-directional motion vector field. 
    \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/motion_vectors_definitive.svg}{Example}.
    
  \item
    Frame prediction. For each block $b$:
    
  \item
    Compute
    \begin{equation}
      \hat{b}\leftarrow \frac{b_i\big(\overleftarrow{e}_{\text{max}}-\overleftarrow{e}(b)\big) + b_k\big(\overrightarrow{e}_{\text{max}}-\overrightarrow{e}(b)\big)}{\big(\overleftarrow{e}_{\text{max}}-\overleftarrow{e}(b)\big) + \big(\overrightarrow{e}_{\text{max}}-\overrightarrow{e}(b)\big)},
    \end{equation}
    
    where $\overleftarrow{e}(b)$ is the (minimum) distortion of the
    best backward matching for block $b$, $\overrightarrow{e}(b)$
    the (minimum) distortion of the best forward matching for block
    $b$,
    $\overleftarrow{e}_{\text{max}}=\overrightarrow{e}_{\text{max}}$ are
    the backward and forward maximum matching distortions, $b_i$ is
    the (backward) block found (as the most similar to $b$) in frame
    $s_i$ and $b_k$ is the (forward) block found in frame
    $s_k$. Notice that, if
    $\overleftarrow{e}(b)=\overrightarrow{e}(b)$, then the
    prediction is
    \begin{equation}
      \hat{b} = \frac{b_i + b_k}{2},
    \end{equation}
    and if $\overleftarrow{e}(b)=0$,
    \begin{equation}
      \hat{b} = b_k,
    \end{equation} and viceversa.
  \end{enumerate}
\end{enumerate}

\subsection*{Lab}
Implement the Section \ref{sec:linear_frame_interpolation} (work on
\url{https://github.com/Sistemas-Multimedia/MCDWT/blob/master/transform/mc/block/interpolate.py}).
Use
\url{https://github.com/Sistemas-Multimedia/MCDWT/blob/master/mcdwt/mc/block/interpolate.py}
and
\url{https://github.com/vicente-gonzalez-ruiz/MCTF-video-coding/blob/master/src/motion\_estimate.cpp}
as reference.

\subsection*{Lab}
Compare the performance of the proposed matching strategies (MSE, MAE
and EE) in the Section \ref{sec:linear_frame_interpolation}, by computing
the variance of the prediction error between the original frame
($s_j$) and the prediction frame ($\hat{s}_j$).

\subsection*{Lab}
Test different DWT filters in the
Section \ref{sec:linear_frame_interpolation} and compare their performance.
Compute the prediction error between the original frame ($s_j$) and
the prediction frame ($\hat{s}_j$). Measure the dependency between
this performance and the distance between frames ($i$, $j$, and
$k$ indexes).

\subsection*{Lab}
Test the use of both the luma and the chroma in
Section \ref{sec:linear_frame_interpolation}, and measure the performance of
each option (only luma vs.~all components), by computing the prediction
error between the original frame ($s_j$) and the prediction frame
($\hat{s}_j$). Measure the dependency of the results with the distance
between frames ($i$, $j$, and $k$ indexes).

\subsection*{Lab}
Analyze the impact of the $R$ (search range) parameter in the
Section \ref{sec:linear_frame_interpolation}. Compute the prediction error
between the original frame ($s_j$) and the prediction frame
($\hat{s}_j$). Study the impact of initializing the motion vectors
(Section \ref{telescopic_search}). Measure the dependency with the
distance between frames ($i$, $j$, and $k$ indexes).

\href{https://nbviewer.jupyter.org/github/vicente-gonzalez-ruiz/video_coding/blob/master/search_range.ipynb}{IPython notebook}

\subsection*{Lab}
Analyze the impact of the $O$ (overlaping) parameter in the
Section \ref{linear_frame_interpolation}, by means of computing the
prediction error between the original frame ($s_j$) and the prediction
frame ($\hat{s}_j$). Measure the dependency with the distance between
frames ($i$, $j$, and $k$ indexes).

\subsection*{Lab}
Analyze the impact of the $B$ (block size) parameter in the
Section \ref{linear_frame_interpolation}, by computing the prediction
error between the original frame ($s_j$) and the prediction frame
($\hat{s}_j$). Compute the expected size of the motion fields using
their 0-order entropy. Measure the dependency with the distance between
frames ($i$, $j$, and $k$ indexes).

\subsection*{Lab}
Analyze the impact of the $A$ (subpixel accuracy) parameter in the
Section \ref{linear_frame_interpolation}, by computing the prediction
error between the original frame ($s_j$) and the prediction frame
($\hat{s}_j$). Compute the expected size of the motion fields using
their entropy. Measure the dependency with the distance between frames
($i$, $j$, and $k$ indexes).

\href{https://nbviewer.jupyter.org/github/vicente-gonzalez-ruiz/video_coding/blob/master/subpixel_accuracy.ipynb}{IPython notebook}

\subsection*{Lab}
Compare the performance of the Section \ref{linear_frame_interpolation}
when it holds that

\begin{equation}
   \hat{b} = \frac{b_i + b_k}{2},
\end{equation}

for all blocks.

\section{MC/DWT hybrid coding alternatives}
\begin{itemize}
\item
  \textbf{t+2d}: The sequence of images is decorrelated first along the
  time (t) and the residue images are compressed, exploiting the
  remaining spatial (2d) redundancy. Examples: MPEG* and H.26* codecs
  (except H.264/SVC).
\item
  \textbf{2d+t}: The spatial (2d) redudancy is explited first (using
  typically the DWT) and after that, the coefficients are decorrelated
  along the time (t). For now, this has only been an experimental setup
  because most DWT transformed domains are not invariant to the
  displacement, and therefore, ME/MC can not be directly applied.
\item
  \textbf{2d+t+2d}: The fist step creates a Laplacian Pyramid (2d),
  which is invariant to the displacement. Next, each level of the
  pyramid is decorrelated along the time (t) and finally, the
  remaining spatial redundancy is removed (2d). The
  Fig~\ref{fig:H264-S-SVC} show an example for H.264/SVC.
\end{itemize}

\begin{figure}
  \svg{H264-S-SVC}{500}
  \caption{SVC scheme in H.264.}
  \label{fig:H264-S-SVC}
\end{figure}

\section{Deblocking filtering}
\begin{itemize}
\tightlist
\item
  If any other block-overlaping techniques have not been applied,
  block-based video encoders improve their performance if a deblocking
  filter in used to create the quantized prediction predictions (see
  the Fig.~\ref{fig:350px-Deblock1}).
\end{itemize}

\begin{figure}
  \jpg{350px-Deblock1}{800}
  \caption{Deblocking filetering effect.}
  \label{fig:350px-Deblock1}
\end{figure}

\begin{itemize}
\tightlist
\item
  The low-pass filter is applied only on the block boundaries.
\end{itemize}

\section{Bit-rate allocation}
\begin{itemize}
\item
  VBR: Under a constant quantization level (constant video quality),
  the number of bits that each compressed image needs depends on the
  image content (Variable Bit-Rate). In the
  Fig.~\ref{fig:closed-loop-1_ir} there is an
  example.
  \begin{figure}
    \svg{closed-loop-1_ir}{500}
    \caption{Example of variable bit-allocation.}
    \label{fig:closed-loop-1_ir}
  \end{figure}
  
\item
  CBR: Using a Constant Bit-Rate strategy, all frames need the same
  space. In the
  Fig.~\ref{fig:CBR} there is an
  example.
  \begin{figure}
    \svg{CBR}{500}
    \caption{Example of constant bit-allocation.}
    \label{CBR}
  \end{figure}
\end{itemize}

\section{Video scalability}
\subsection{Quality scalability}

\begin{figure}
  \svg{quality-scalability}{1000}
  \caption{Quality scalability.}
  \label{fig:quality-scalability}
\end{figure}

Véase la Fig.~\ref{fig:quality-scalability}.
\begin{itemize}
\item
  Ideal for remote visualization environments.
\item
  By definition, $V^{[0]}:=V$.
\end{itemize}

\subsection{Temporal scalability}
\begin{figure}
  \svg{temporal-scalability}{1000}
  \caption{Temporal scalability.}
  \label{fig:temporal-scalability}
\end{figure}

Véase la Fig.~\ref{fig:temporal-scalability}.

\begin{itemize}
\item
  It holds that

  \begin{equation}
    V^{t}=\{V_{2^t i}\}=\{V_{2i}^{t-1}\},
  \end{equation}

  where $t$ denotes the Temporal Resolution Level (TRL).
\item
  Notice that $V:=V^{0}$.
\item
  Useful for fast random access.
\end{itemize}

%\bibliography{video-compression}
%\end{document}

\subsection{\href{http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/videowavelet_UCB1-3.pdf}{Spatial scalability}}

\begin{figure}
  \svg{spatial-scalability}{1000}
  \caption{Spatial scalability.}
  \label{fig:spatial-scalability}
\end{figure}

Véase la Fig.~\ref{fig:spatial-scalability}.

\begin{itemize}
\item
  Useful for low-resolution devices.
\item
  By definition, $V_i:=V_i^{<0>}$ and $V_i^{<s>}$ has a
  $\frac{Y}{2^s}\times \frac{X}{2^s}$ resolution, where $X\times Y$
  is the resolution of $V_i$.
\end{itemize}

\bibliography{image_pyramids,DWT,motion_estimation,HEVC,JPEG2000,video_compression}
